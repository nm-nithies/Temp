def fix_dynamic_shape(model_def, input_shapes):
    graph_inputs = model_def.graph.input
    initializers = model_def.graph.initializer
    initializer_names = [n.name for n in initializers]
    for i, ipvi in enumerate(graph_inputs):
        shape = [int(n.dim_value) for n in list(ipvi.type.tensor_type.shape.dim)]
        if 0 in shape:
            ip_name = ipvi.name
            if ip_name in initializer_names:
                continue  # avoid changing empty tensor dim for Resize node etc., e.g. nanodet_plus_m_416 model
            elem_type = ipvi.type.tensor_type.elem_type
            given_shape = input_shapes.get(ip_name, None)
            if given_shape is None:
                zero_idx = [i for i in range(len(shape)) if shape[i] == 0]
                for ind in zero_idx:
                    shape[ind] = 1
                    logger.info("[INFO] Set the dynamic dim {} to 1 for {}.".format(ind, ip_name))
            else:
                shape = given_shape
                logger.info("[INFO] Set the dynamic shape to {} for {}.".format(shape, ip_name))
            new_ipvi = helper.make_tensor_value_info(ip_name, elem_type, shape)
            graph_inputs[i].CopyFrom(new_ipvi)
            logger.debug("[DEBUG] Fixed input:\n{}".format(graph_inputs[i]))
    return model_def
