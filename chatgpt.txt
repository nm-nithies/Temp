import torch
import torch.nn as nn

class AddConvReluSoftmax(nn.Module):
    def __init__(self, input2_shape):
        super(AddConvReluSoftmax, self).__init__()
        self.input2_shape = input2_shape
        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x1, x2):
        # Broadcast input2 if needed
        if x2.ndim == 0 or x2.shape != x1.shape:
            x2 = x2.expand_as(x1)
        out = x1 + x2
        out = self.conv(out)
        out = self.relu(out)
        out = self.softmax(out)
        return out
