import torch
import torch.nn as nn

class AddConvReluSoftmax(nn.Module):
    def __init__(self, input2_shape):
        super(AddConvReluSoftmax, self).__init__()
        self.input2_shape = input2_shape
        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x1, x2):
        # Broadcast input2 if needed
        if x2.ndim == 0 or x2.shape != x1.shape:
            x2 = x2.expand_as(x1)
        out = x1 + x2
        out = self.conv(out)
        out = self.relu(out)
        out = self.softmax(out)
        return out

model1 = AddConvReluSoftmax(input2_shape=torch.Size([]))
model1.eval()

x1 = torch.randn(1, 3, 256, 256)
x2 = torch.tensor(2.0)  # scalar input

torch.onnx.export(
    model1,
    (x1, x2),
    "model1.onnx",
    input_names=["input1", "input2"],
    output_names=["output"],
    opset_version=11,
    dynamic_axes={"input1": {0: "batch"}, "output": {0: "batch"}}
)

model2 = AddConvReluSoftmax(input2_shape=torch.Size([1, 3, 256, 256]))
model2.eval()

x1 = torch.randn(1, 3, 256, 256)
x2 = torch.randn(1, 3, 256, 256)

torch.onnx.export(
    model2,
    (x1, x2),
    "model2.onnx",
    input_names=["input1", "input2"],
    output_names=["output"],
    opset_version=11,
    dynamic_axes={"input1": {0: "batch"}, "input2": {0: "batch"}, "output": {0: "batch"}}
)
