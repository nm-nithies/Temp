# Copyright 2023-2025 Synopsys, Inc.
# This Synopsys software and all associated documentation are proprietary
# to Synopsys, Inc. and may only be used pursuant to the terms and conditions
# of a written license agreement with Synopsys, Inc.
# All other use, reproduction, modification, or distribution of the Synopsys
# software or the associated documentation is strictly prohibited.


import networkx as nx
import numpy as np
from onnx import OperatorSetIdProto


from nnac.core.log import Logger
from .single_layer_transforms import fetch_succ_layer0, check_layer_type, get_constant_value
from nnac.core.customized_local_functions.rms_normalization import RMSNormalization

logger = Logger("OPTIMIZATION")


"""
This pass fuses primitive operators into RMSNormalization.

Pattern1:

             A
             |______
             |      |
             |    Pow (Y=2)
             |       |
             |    ReduceMean
             |       |
             |      Add (epsilon)
              |      |
               |    Sqrt
                |    |
                 |   Div
                  |  |
                   Mul
                    |
                   Mul (gamma)
                    |
                    B

"""


def FusePrimitivesIntoRMSNorm(opt):
    rms_pattern1(opt)
    rms_pattern2(opt)


def rms_pattern2(opt):
    G = opt.G
    tensor_dict = opt.TensorDict
    shape_dict = opt.ShapeDict
    type_dict = opt.TypeDict
    graph_attrs = opt.graph_attrs
    layers = list(nx.topological_sort(G))
    for layer in layers:
        if layer not in G.nodes:
            continue
        remove_layers = []
        layer_succs =  list(G.successors(layer))
        if len(layer_succs) != 2:
            continue

        succs_optype = []
        for node in layer_succs:
            succs_optype.append(G.nodes[node].get("op_type", None))

        if sorted(succs_optype) != ["Mul", "ReduceL2"]:
            continue

        if fetch_succ_layer0(G, layer_succs[0]) != fetch_succ_layer0(G, layer_succs[1]) and \
           not check_layer_type(fetch_succ_layer0(G, layer_succs[0]), "Div"):
            continue

        div_layer = fetch_succ_layer0(G, layer_succs[0])
        remove_layers.extend(layer_succs)
        remove_layers.append(div_layer)

        for node in layer_succs:
            if check_layer_type(G, node, "ReduceL2"):
                axis = None
                if G.nodes[node]["input"][1] in tensor_dict:
                    print(G.nodes[node])
                    axis = tensor_dict[G.nodes[node]["input"][1]]
            else:
                scale = get_constant_value(opt, G.nodes[node]['input']) * (1/np.sqrt(shape_dict[node][1]))

        if scale is None:
            continue
        
        rmsnorm_node = div_layer + "/rmsnorm"
        G.add_node(
        rmsnorm_node,
        **{
            "op_type": "RMSNormalizations",
            "input": [
                layer,
                rmsnorm_node + "/rmsnorm_scale",
            ],
            "output": [
                rmsnorm_node
            ],
            "attr_dict": {"axis": -1, "epsilon": 1e-5},
            "domain": "mmcv",
        }
        )
        tensor_dict[rmsnorm_node + "/rmsnorm_scale"] = scale
        type_dict[rmsnorm_node] = type_dict[div_layer]
        shape_dict[rmsnorm_node] = shape_dict[div_layer]
        opt.compare_dict[rmsnorm_node] = opt.compare_dict[div_layer]


        G.add_edge(layer, rmsnorm_node)
        if len(list(G.successors(div_layer))) > 0:
            for output in list(G.successors(div_layer)):
                for succ_input in range(len(G.nodes[output]["input"])):
                    if G.nodes[output]['input'][succ_input] == div_layer:
                        G.nodes[output]['input'][succ_input] = rmsnorm_node

                G.add_edge(rmsnorm_node, output)

        for n in remove_layers:
            G.remove_node(n)
        opt.passes_counter["FusePrimitivesIntoRMSNorm"] += 1

        # cur_doms = [d.domain for d in graph_attrs["opset_import"]]
        # if "snps.onnx.local" not in cur_doms:
        #     graph_attrs["opset_import"].append(
        #         OperatorSetIdProto(version=1, domain="snps.onnx.local")
        #     )

        # functions_list = graph_attrs.get("functions", [])
        # func_obj = RMSNormalization.to_function_proto()
        # if func_obj.name not in [func.name for func in functions_list]:
        #     functions_list.append(func_obj)


        # graph_attrs["functions"] = functions_list


def rms_pattern1(opt):
    G = opt.G
    tensor_dict = opt.TensorDict
    shape_dict = opt.ShapeDict
    layers = list(nx.topological_sort(G))

    for layer in layers:
        remove_layers = []
        if layer  not in G.nodes or G.nodes[layer].get("op_type", None) != "Pow":
            continue

        pow_inputs = G.nodes[layer]["input"]
        exp = tensor_dict[pow_inputs[1]]
        if exp != 2:
            continue

        pow_succs = list(G.successors(layer))
        if len(pow_succs) != 1 or G.nodes[pow_succs[0]]["op_type"] != "ReduceMean":
            continue

        remove_layers.append(layer)
        mean_layer = pow_succs[0]
        mean_layer_out_shape = shape_dict[mean_layer]

        axis = None
        if G.nodes[mean_layer]["input"][1] in tensor_dict:
            axis = tensor_dict[G.nodes[mean_layer]["input"][1]]
        if axis is None:
            logger.debug("[DEBUG] Can not find pattern axis in tensor_dict.")
            continue
        
        if axis < 0:
            input_rank = len(shape_dict[layer])
            axis = axis + input_rank

        remove_layers.append(mean_layer)
        mean_succs = list(G.successors(mean_layer))
        if len(mean_succs) != 1 or G.nodes[mean_succs[0]].get("op_type", None) != "Add":
            continue
        add_layer = mean_succs[0]
        eps = None

        add_layer = mean_succs[0]
        for input in G.nodes[add_layer]["input"]:
            if input in tensor_dict:
                eps = float(np.mean(tensor_dict[input]))
                break

        remove_layers.append(add_layer)
        add_succs = list(G.successors(add_layer))
        if len(mean_succs) != 1 or G.nodes[add_succs[0]].get("op_type", None) != "Sqrt":
            continue
        sqrt_layer = add_succs[0]

        remove_layers.append(sqrt_layer)
        sqrt_succs = list(G.successors(sqrt_layer))
        if len(sqrt_succs) != 1 or G.nodes[sqrt_succs[0]].get("op_type", None) not in ["Div", "Reciprocal"]:
            continue

        if G.nodes[sqrt_succs[0]].get("op_type", None) == "Div":
            if G.nodes[sqrt_succs[0]]['input'][0] not in tensor_dict or tensor_dict[G.nodes[sqrt_succs[0]]['input'][0]] != 1:
                continue

        div_or_reciprocal_layer = sqrt_succs[0]
        remove_layers.append(div_or_reciprocal_layer)
        div_succs = list(G.successors(div_or_reciprocal_layer))
        if len(div_succs) != 1 or G.nodes[div_succs[0]].get("op_type", None) != "Mul":
            continue

        mul_layer1 = div_succs[0]
        remove_layers.append(mul_layer1)

        rms_input = G.nodes[layer]['input'][0]
        rms_output = mul_layer1
        rms_nodes = remove_layers
        rms_output_succs = list(G.successors(rms_output))
        mul_layer1_succs = list(G.successors(mul_layer1))

        rmsnorm_node = add_rmsnorm_to_graph(opt, rms_input, rms_output, rms_nodes, axis, eps, rms_output_succs)
        if len(mul_layer1_succs) != 1 or G.nodes[mul_layer1_succs[0]].get("op_type", None) != "Mul":
            continue

        # mul_layer2 = mul_layer1_succs[0]
        # if fuse_mul_into_ln(opt, rmsnorm_node, mul_layer2):
        #     opt.compare_dict[rmsnorm_node] = opt.compare_dict[mul_layer2]
        #     G.remove_node(mul_layer2)


def add_rmsnorm_to_graph(opt, rms_input, rms_output, rms_nodes, axis, eps, rms_output_succs):
    G = opt.G
    tensor_dict = opt.TensorDict
    shape_dict = opt.ShapeDict
    type_dict = opt.TypeDict
    graph_attrs = opt.graph_attrs

    rms_name = rms_output
    rmsnorm_node = rms_name + "/rmsnorm"

    input_shape = shape_dict[rms_input]
    shape_dict[rmsnorm_node] = input_shape

    norm_shape = shape_dict[rmsnorm_node][2:]
    tensor_dict[rms_name + "/rmsnorm_scale"] = np.ones(norm_shape, dtype=np.float32)

    type_dict[rmsnorm_node] = type_dict[rms_name]

    G.add_node(
        rmsnorm_node,
        **{
            "op_type": "RMSNormalizations",
            "input": [
                rms_input,
                rms_name + "/rmsnorm_scale",
            ],
            "output": [
                rmsnorm_node
            ],
            "attr_dict": {"axis": -1, "epsilon": eps},
            "domain": "mmcv",
        }
    )

    for n in set(rms_nodes):
        G.remove_node(n)
    for input in rms_input:
        G.add_edge(input, rmsnorm_node)
    for output in rms_output_succs:
        for succ_input in range(len(G.nodes[output]["input"])):
            if G.nodes[output]['input'][succ_input] == rms_output:
                G.nodes[output]['input'][succ_input] = rmsnorm_node

        G.add_edge(rmsnorm_node, output)
    opt.compare_dict[rmsnorm_node] = opt.compare_dict[rms_output]
    opt.passes_counter["FusePrimitivesIntoRMSNorm"] += 1

    # cur_doms = [d.domain for d in graph_attrs["opset_import"]]
    # if "snps.onnx.local" not in cur_doms:
    #     graph_attrs["opset_import"].append(
    #         OperatorSetIdProto(version=1, domain="snps.onnx.local")
    #     )

    # functions_list = graph_attrs.get("functions", [])
    # func_obj = RMSNormalization.to_function_proto()
    # if func_obj.name not in [func.name for func in functions_list]:
    #     functions_list.append(func_obj)

    # graph_attrs["functions"] = functions_list

    return rmsnorm_node


def fuse_mul_into_ln(opt, rms_node, mul_layer):
    G = opt.G
    TensorDict = opt.TensorDict
    rms_inputs = G.nodes[rms_node]["input"]
    scale = TensorDict.get(rms_inputs[1])

    mul_scale = get_scale(opt, mul_layer)
    if mul_scale is None:
        logger.debug(
            "Failed to fuse Mul {} into RMSNorm {}, it's not a constant multiplication.".format(
                mul_layer, rms_node
            )
        )
        return False

    if mul_scale.size != scale.size:
        logger.debug(
            "Failed to fuse Mul {} into RMSNorm {}, the scale size {} v.s. {} doesn't match.".format(
                mul_layer, rms_node, mul_scale.size, scale.size
            )
        )
        return False
    fused_scale = scale * mul_scale
    fused_scale_name = rms_inputs[1] + '/fused_mul'
    TensorDict[fused_scale_name] = fused_scale
    del TensorDict[rms_inputs[1]]
    rms_inputs[1] = fused_scale_name

    logger.debug(
        "Fuse Mul {} into RMSNorm {}.".format(
            mul_layer, rms_node
        )
    )
    return True


def get_scale(opt, layer):
    G = opt.G
    TensorDict = opt.TensorDict
    for input in G.nodes[layer]["input"]:
        if input in TensorDict:
            return TensorDict[input]
    return None
